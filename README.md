#  Aurora-M

This is code to train and run Aurora-M, a starcoderplus model trained on 200B additional tokens of multilingual and multidomain data, and adapted for multimodal understanding using the BakLLaVA/LLaVA 1.5 code base. This model is intended for MoE adapation.

![Aurora Over BakLLaVA](https://github.com/ontocord/aurora-m/blob/main/Aurora_over_bakllava.png?raw=true)

Compute provided by the LUMI Supercomputers and JUWELS Supercomptuers. This is part of the [M*DEL](https://huggingface.co/Multi-Domain-Expert-Learning) project. See our page for all the participants. 
