#  Aurora-M

This is code to train and run Aurora-M, a starcoderplus model trained on 200B additional tokens of multilingual and multidomain data, and adapted for multimodal understanding using the BakLLaVA/LLaVA 1.5 code base. This model is intended for MoE adapation experimentation.

![Aurora Over BakLLaVA](https://github.com/ontocord/aurora-m/blob/main/Aurora_over_bakllava.png?raw=true)

Compute provided by the LUMI Supercomputer center and JUWELS Supercomptuer center. This is part of the [M*DEL](https://huggingface.co/Multi-Domain-Expert-Learning) project. See our page for all the participants. 

Check out our BakLLaVA project, which is a cooperation between A project in collaboration with [LAION](www.laion.ai), [Ontocord](www.ontocord.ai) and [Skunkworks OSS AI group](https://huggingface.co/SkunkworksAI).
