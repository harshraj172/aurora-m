#  Aurora-M

This starcoderplus model trained on 200B additional tokens of multilingual and multidomain data, and adapted for multimodal understanding using the BakLLaVA/LLaVA 1.5 code base. This model is intended for MoE adapation.

![Aurora Over BakLLaVA](https://github.com/ontocord/aurora-m/blob/main/image.jpg?raw=true)

