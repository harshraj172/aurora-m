#  Aurora-M

This is code to finetune and run Aurora-M, an open source Starcoderplus based model trained on 200B additional tokens of multilingual and multidomain data, and adapted for multimodal understanding using the BakLLaVA/LLaVA 1.5 code base. The 200B additional tokens were trained with [BigCode's Megatron fork](https://github.com/bigcode-project/Megatron-LM). This model is intended for mixture of experts (MoE) adapation using the [M*DEL](https://huggingface.co/Multi-Domain-Expert-Learning) MoE adapatation. See our M*DEL project page for more details. 

Compute provided by the LUMI Supercomputer center and JUWELS Supercomptuer center. Thank you!

Also check out our BakLLaVA project, which is a cooperation between the AI Open source organizations: [LAION](https://laion.ai), [Ontocord](https://ontocord.ai), [Skunkworks OSS AI group](https://huggingface.co/SkunkworksAI) and [AI Alignment Lab](https://github.com/Alignment-Lab-AI).

![Aurora Over BakLLaVA](https://github.com/ontocord/aurora-m/blob/main/Aurora_over_bakllava.png?raw=true)
